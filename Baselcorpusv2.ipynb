{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basel III Transformer (Raschka-style) — Pretrain → Finetune + Baseline & Metrics\n",
    "\n",
    "**Author:** Veena  \n",
    "**Date:** 2025-09-28\n",
    "\n",
    "This notebook:\n",
    "1. Loads **`basel3_corpus.txt`** (≥10k tokens, Basel III–style original text)\n",
    "2. **Pretrains** a GPT-like mini-Transformer on next-word prediction (causal LM)\n",
    "3. **Finetunes** a classification head on labeled clauses (Liquidity / Capital / Leverage)\n",
    "4. Adds a **Baseline**: Bag-of-Words + Logistic Regression (scikit-learn)\n",
    "5. Reports **Precision / Recall / F1** on a held-out **validation split**\n",
    "6. **Saves plots** to `assets/` for LinkedIn: LM loss, CLS loss, confusion matrix\n",
    "\n",
    "> Audit-friendly: fixed seeds, readable code, minimal magic; easy to reproduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install torch scikit-learn --index-url https://download.pytorch.org/whl/cpu >/dev/null 2>&1 || echo \"Deps preinstalled\"\n",
    "import math, os, time, json, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ASSETS_DIR = \"assets\"\n",
    "os.makedirs(ASSETS_DIR, exist_ok=True)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Corpus & Build Labeled Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Basel III corpus from text file (one line per clause/paragraph)\n",
    "with open(\"basel3_corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lm_texts = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "# Heuristic labeling for finetune classes\n",
    "liquidity_kw = [\"liquidity\", \"outflow\", \"stable funding\", \"nsfr\", \"lcr\"]\n",
    "capital_kw   = [\"capital\", \"tier one\", \"tier two\", \"rwa\", \"buffer\", \"conservation\"]\n",
    "leverage_kw  = [\"leverage\", \"exposure measure\", \"non risk based\", \"backstop\"]\n",
    "\n",
    "def pick_label(text):\n",
    "    lo = text.lower()\n",
    "    if any(k in lo for k in liquidity_kw): return \"Liquidity\"\n",
    "    if any(k in lo for k in capital_kw):   return \"Capital\"\n",
    "    if any(k in lo for k in leverage_kw):  return \"Leverage\"\n",
    "    return None\n",
    "\n",
    "cls_pool = {\"Liquidity\":[], \"Capital\":[], \"Leverage\":[]}\n",
    "for t in lm_texts:\n",
    "    lab = pick_label(t)\n",
    "    if lab and len(cls_pool[lab]) < 120:  # collect up to 120 per class\n",
    "        cls_pool[lab].append(t)\n",
    "\n",
    "# Build dataset\n",
    "cls_data = []\n",
    "for lab in [\"Liquidity\",\"Capital\",\"Leverage\"]:\n",
    "    for s in cls_pool[lab]:\n",
    "        cls_data.append((lab, s))\n",
    "\n",
    "labels = sorted(set([lab for lab,_ in cls_data]))\n",
    "label_to_id = {lab:i for i,lab in enumerate(labels)}\n",
    "num_classes = len(labels)\n",
    "\n",
    "print(\"LM lines:\", len(lm_texts))\n",
    "print(\"Class counts:\", {k:len(v) for k,v in cls_pool.items()})\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenizer (Simple BPE with HuggingFace Tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "\n",
    "tok = Tokenizer(models.BPE())\n",
    "trainer = trainers.BpeTrainer(vocab_size=5000, special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\"])\n",
    "tok.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tok.train_from_iterator(lm_texts, trainer=trainer)\n",
    "\n",
    "VOCAB_SIZE = tok.get_vocab_size()\n",
    "print(\"Tokenizer vocab size:\", VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformer Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.qkv = nn.Linear(d_model, d_model*3)\n",
    "        self.o = nn.Linear(d_model, d_model)\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.size()\n",
    "        qkv = self.qkv(x).reshape(B,T,self.num_heads,3*self.d_k)\n",
    "        q,k,v = qkv.split(self.d_k, dim=-1)\n",
    "        q,k,v = [t.transpose(1,2) for t in (q,k,v)]\n",
    "        att = (q @ k.transpose(-2,-1)) / math.sqrt(self.d_k)\n",
    "        mask = torch.triu(torch.ones(T,T, device=x.device), diagonal=1)\n",
    "        att = att.masked_fill(mask==1, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        z = (att @ v).transpose(1,2).reshape(B,T,C)\n",
    "        return self.o(z)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model,d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff,d_model)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTMini(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, num_heads=4, d_ff=512, num_layers=2, max_len=64):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(d_model,num_heads,d_ff) for _ in range(num_layers)])\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "    def forward(self, idx):\n",
    "        B,T = idx.size()\n",
    "        pos = torch.arange(T, device=idx.device)\n",
    "        x = self.emb(idx) + self.pos_emb(pos)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        return self.head(x)\n",
    "\n",
    "LM_VOCAB = VOCAB_SIZE\n",
    "model = GPTMini(LM_VOCAB).to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, \"M params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pretraining — Next-Word Prediction (Language Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self, texts, tok, max_len=128):\n",
    "        self.data = []\n",
    "        for t in texts:\n",
    "            ids = tok.encode(t).ids\n",
    "            # create many (prefix -> next token) sequences up to max_len\n",
    "            for i in range(1, min(len(ids), max_len)):\n",
    "                inp = ids[:i]\n",
    "                tgt = ids[1:i+1]\n",
    "                self.data.append((inp, tgt))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        inp, tgt = self.data[idx]\n",
    "        x = torch.tensor(inp, dtype=torch.long)\n",
    "        y = torch.tensor(tgt, dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "def collate_lm(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    maxlen = max(len(x) for x in xs)\n",
    "    X = torch.zeros(len(xs), maxlen, dtype=torch.long)\n",
    "    Y = torch.zeros(len(xs), maxlen, dtype=torch.long)\n",
    "    for i, (x, y) in enumerate(zip(xs, ys)):\n",
    "        X[i, :len(x)] = x\n",
    "        Y[i, :len(y)] = y\n",
    "    return X, Y\n",
    "\n",
    "# use a subset for quick demo; increase for better quality\n",
    "lm_ds = LMDataset(lm_texts, tok, max_len=128)\n",
    "lm_dl = DataLoader(lm_ds, batch_size=64, shuffle=True, collate_fn=collate_lm)\n",
    "\n",
    "opt_lm = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "lm_losses = []\n",
    "epochs_lm = 8\n",
    "for ep in range(epochs_lm):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for xb, yb in lm_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), yb.reshape(-1))\n",
    "        opt_lm.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt_lm.step()\n",
    "        running += loss.item()\n",
    "    lm_losses.append(running / max(1, len(lm_dl)))\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(lm_losses)\n",
    "plt.title(\"LM Training Loss (Next-Word Prediction)\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.tight_layout()\n",
    "plt.savefig(f\"{ASSETS_DIR}/lm_loss.png\", dpi=160)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Datasets & Split for Classification (Liquidity / Capital / Leverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLSDataset(Dataset):\n",
    "    def __init__(self, pairs, tok, max_len=128):\n",
    "        self.items = []\n",
    "        for lab, txt in pairs:\n",
    "            ids = tok.encode(txt).ids[:max_len]\n",
    "            self.items.append((ids, label_to_id[lab]))\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    def __getitem__(self, idx):\n",
    "        ids, lab = self.items[idx]\n",
    "        return torch.tensor(ids, dtype=torch.long), torch.tensor(lab, dtype=torch.long)\n",
    "\n",
    "def collate_cls(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    maxlen = max(len(x) for x in xs)\n",
    "    X = torch.zeros(len(xs), maxlen, dtype=torch.long)\n",
    "    for i, x in enumerate(xs):\n",
    "        X[i, :len(x)] = x\n",
    "    Y = torch.tensor(ys, dtype=torch.long)\n",
    "    return X, Y\n",
    "\n",
    "# 80/20 random split (for exact reproducibility, persist indices externally)\n",
    "n = len(cls_data)\n",
    "idx = list(range(n))\n",
    "random.shuffle(idx)\n",
    "n_train = int(0.8 * n)\n",
    "train_idx = idx[:n_train]\n",
    "val_idx = idx[n_train:]\n",
    "\n",
    "train_pairs = [cls_data[i] for i in train_idx]\n",
    "val_pairs   = [cls_data[i] for i in val_idx]\n",
    "\n",
    "cls_train = CLSDataset(train_pairs, tok, max_len=128)\n",
    "cls_val   = CLSDataset(val_pairs, tok, max_len=128)\n",
    "\n",
    "train_dl = DataLoader(cls_train, batch_size=16, shuffle=True,  collate_fn=collate_cls)\n",
    "val_dl   = DataLoader(cls_val,   batch_size=16, shuffle=False, collate_fn=collate_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classification Model (uses Transformer as encoder → mean-pool → linear head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTForClassification(nn.Module):\n",
    "    def __init__(self, gpt, num_classes):\n",
    "        super().__init__()\n",
    "        self.gpt = gpt\n",
    "        # tap hidden states by re-running forward up to the layer norm\n",
    "        self.cls = nn.Linear(gpt.head.out_features, num_classes)\n",
    "    def forward(self, x):\n",
    "        # Recompute to get hidden states just before LM head\n",
    "        B, T = x.size()\n",
    "        h = self.gpt.emb(x) + self.gpt.pos_emb(torch.arange(T, device=x.device))\n",
    "        h = self.gpt.blocks(h)\n",
    "        h = self.gpt.ln(h)\n",
    "        pooled = h.mean(dim=1)\n",
    "        return self.cls(pooled)\n",
    "\n",
    "clf = GPTForClassification(model, num_classes).to(device)\n",
    "opt_cls = torch.optim.AdamW(clf.parameters(), lr=5e-4)\n",
    "cls_losses = []\n",
    "epochs_cls = 30\n",
    "for ep in range(epochs_cls):\n",
    "    clf.train()\n",
    "    running = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = clf(xb)\n",
    "        loss = F.cross_entropy(logits, yb)\n",
    "        opt_cls.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(clf.parameters(), 1.0)\n",
    "        opt_cls.step()\n",
    "        running += loss.item()\n",
    "    cls_losses.append(running / max(1, len(train_dl)))\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(cls_losses)\n",
    "plt.title(\"Classification Fine-tune Loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.tight_layout()\n",
    "plt.savefig(f\"{ASSETS_DIR}/cls_loss.png\", dpi=160)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation Metrics (Precision / Recall / F1) + Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_dl:\n",
    "        xb = xb.to(device)\n",
    "        logits = clf(xb)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=list(range(num_classes)), average='weighted', zero_division=0)\n",
    "print(\"Weighted Precision:\", round(prec,3), \"Recall:\", round(rec,3), \"F1:\", round(f1,3))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "plt.figure(figsize=(4.5,4.5))\n",
    "plt.imshow(cm, aspect='auto')\n",
    "plt.title(\"Confusion Matrix (Validation)\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.xticks(range(num_classes), labels, rotation=45)\n",
    "plt.yticks(range(num_classes), labels)\n",
    "for i in range(num_classes):\n",
    "    for j in range(num_classes):\n",
    "        plt.text(j, i, int(cm[i,j]), ha='center', va='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{ASSETS_DIR}/cls_confusion.png\", dpi=160)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Baseline — Bag-of-Words + Logistic Regression (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vec = CountVectorizer(max_features=20000, ngram_range=(1,2))\n",
    "X_all = vec.fit_transform([t for _, t in cls_data])\n",
    "y_all = np.array([label_to_id[l] for l, _ in cls_data])\n",
    "Xtr, Xva, ytr, yva = train_test_split(X_all, y_all, test_size=0.2, random_state=42, stratify=y_all)\n",
    "\n",
    "lr = LogisticRegression(max_iter=2000)\n",
    "lr.fit(Xtr, ytr)\n",
    "yp = lr.predict(Xva)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Baseline (BOW + LR) — Validation Report\")\n",
    "print(classification_report(yva, yp, target_names=labels, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Metrics for Audit (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "  \"weighted_precision\": float(round(prec, 3)),\n",
    "  \"weighted_recall\": float(round(rec, 3)),\n",
    "  \"weighted_f1\": float(round(f1, 3)),\n",
    "  \"labels\": labels,\n",
    "  \"notes\": \"Transformer vs baseline reported on the same 80/20 split.\"\n",
    "}\n",
    "with open(f\"{ASSETS_DIR}/metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    import json\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(\"Saved metrics to assets/metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Demo — Helper to Classify a Sentence (for quick checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence(text: str) -> str:\n",
    "    ids = tok.encode(text).ids[:128]\n",
    "    x = torch.tensor([ids], dtype=torch.long).to(device)\n",
    "    clf.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = clf(x)\n",
    "        p = logits.argmax(dim=1).item()\n",
    "    return labels[p]\n",
    "\n",
    "tests = [\n",
    "    \"The CET1 capital ratio fell from 13.1% to 12.9%.\",\n",
    "    \"Liquidity coverage ratios remained well above the 100% minimum.\",\n",
    "    \"Banks reduced leverage exposure through off-balance adjustments.\"\n",
    "]\n",
    "for s in tests:\n",
    "    print(f\"Sentence: {s}\")\n",
    "    print(\" → Predicted:\", classify_sentence(s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
