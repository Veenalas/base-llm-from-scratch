{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basel III Transformer (Raschka-style) — Pretrain → Finetune + Baseline & Metrics\n",
    "\n",
    "**Author:** Veena  \n",
    "**Date:** 2025-09-28\n",
    "\n",
    "This notebook:\n",
    "1. Loads **`basel3_corpus.txt`** (≥10k tokens, Basel III–style original text)\n",
    "2. **Pretrains** a GPT-like mini-Transformer on next-word prediction (causal LM)\n",
    "3. **Finetunes** a classification head on labeled clauses (Liquidity / Capital / Leverage)\n",
    "4. Adds a **Baseline**: Bag-of-Words + Logistic Regression (scikit-learn)\n",
    "5. Reports **Precision / Recall / F1** on a held-out **validation split**\n",
    "6. **Saves plots** to `assets/` for LinkedIn: LM loss, CLS loss, confusion matrix\n",
    "\n",
    "> Audit-friendly: fixed seeds, readable code, minimal magic; easy to reproduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install torch scikit-learn --index-url https://download.pytorch.org/whl/cpu >/dev/null 2>&1 || echo \"Deps preinstalled\"\n",
    "import math, os, time, json, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ASSETS_DIR = \"assets\"\n",
    "os.makedirs(ASSETS_DIR, exist_ok=True)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Corpus & Build Labeled Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Basel III corpus from text file (one line per clause/paragraph)\n",
    "with open(\"basel3_corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lm_texts = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "# Heuristic labeling for finetune classes\n",
    "liquidity_kw = [\"liquidity\", \"outflow\", \"stable funding\", \"nsfr\", \"lcr\"]\n",
    "capital_kw   = [\"capital\", \"tier one\", \"tier two\", \"rwa\", \"buffer\", \"conservation\"]\n",
    "leverage_kw  = [\"leverage\", \"exposure measure\", \"non risk based\", \"backstop\"]\n",
    "\n",
    "def pick_label(text):\n",
    "    lo = text.lower()\n",
    "    if any(k in lo for k in liquidity_kw): return \"Liquidity\"\n",
    "    if any(k in lo for k in capital_kw):   return \"Capital\"\n",
    "    if any(k in lo for k in leverage_kw):  return \"Leverage\"\n",
    "    return None\n",
    "\n",
    "cls_pool = {\"Liquidity\":[], \"Capital\":[], \"Leverage\":[]}\n",
    "for t in lm_texts:\n",
    "    lab = pick_label(t)\n",
    "    if lab and len(cls_pool[lab]) < 120:  # collect up to 120 per class\n",
    "        cls_pool[lab].append(t)\n",
    "\n",
    "# Build dataset\n",
    "cls_data = []\n",
    "for lab in [\"Liquidity\",\"Capital\",\"Leverage\"]:\n",
    "    for s in cls_pool[lab]:\n",
    "        cls_data.append((lab, s))\n",
    "\n",
    "labels = sorted(set([lab for lab,_ in cls_data]))\n",
    "label_to_id = {lab:i for i,lab in enumerate(labels)}\n",
    "num_classes = len(labels)\n",
    "\n",
    "print(\"LM lines:\", len(lm_texts))\n",
    "print(\"Class counts:\", {k:len(v) for k,v in cls_pool.items()})\n",
    "print(\"Labels:\", labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
},
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenizer (Simple BPE with HuggingFace Tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "\n",
    "tok = Tokenizer(models.BPE())\n",
    "trainer = trainers.BpeTrainer(vocab_size=5000, special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\"])\n",
    "tok.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tok.train_from_iterator(lm_texts, trainer=trainer)\n",
    "\n",
    "VOCAB_SIZE = tok.get_vocab_size()\n",
    "print(\"Tokenizer vocab size:\", VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformer Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.qkv = nn.Linear(d_model, d_model*3)\n",
    "        self.o = nn.Linear(d_model, d_model)\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.size()\n",
    "        qkv = self.qkv(x).reshape(B,T,self.num_heads,3*self.d_k)\n",
    "        q,k,v = qkv.split(self.d_k, dim=-1)\n",
    "        q,k,v = [t.transpose(1,2) for t in (q,k,v)]\n",
    "        att = (q @ k.transpose(-2,-1)) / math.sqrt(self.d_k)\n",
    "        mask = torch.triu(torch.ones(T,T, device=x.device), diagonal=1)\n",
    "        att = att.masked_fill(mask==1, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        z = (att @ v).transpose(1,2).reshape(B,T,C)\n",
    "        return self.o(z)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model,d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff,d_model)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTMini(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, num_heads=4, d_ff=512, num_layers=2, max_len=64):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(d_model,num_heads,d_ff) for _ in range(num_layers)])\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "    def forward(self, idx):\n",
    "        B,T = idx.size()\n",
    "        pos = torch.arange(T, device=idx.device)\n",
    "        x = self.emb(idx) + self.pos_emb(pos)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        return self.head(x)\n",
    "\n",
    "LM_VOCAB = VOCAB_SIZE\n",
    "model = GPTMini(LM_VOCAB).to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, \"M params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pretraining — Next Word Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self, texts, tok, max_len=64):\n",
    "        self.data = []\n",
    "        for t in texts:\n",
    "            ids = tok.encode(t).ids\n",
    "            for i in range(1, min(len(ids),max_len)):\n",
    "                inp = ids[:i]\n",
    "                tgt = ids[1:i+1]\n",
    "                self.data.append((inp,tgt))\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        inp,tgt = self.data[idx]\n",
    "        x = torch.tensor(inp,dtype=torch.long)\n",
    "        y = torch.tensor(tgt,dtype=torch.long)\n",
    "        return x,y\n",
    "\n",
    "def collate(batch, max_len=64):\n",
    "    xs,ys = zip(*batch)\n",
    "    maxlen = max(len(x) for x in xs)\n",
    "    X = torch.zeros(len(xs), maxlen, dtype=torch.long)\n",
    "    Y = torch.zeros(len(xs), maxlen, dtype=torch.long)\n",
    "    for i,(x,y) in enumerate(zip(xs,ys)):\n",
    "        X[i,:len(x)] = x\n",
    "        Y[i,:len(y)] = y\n",
    "    return X,Y\n",
    "\n",
    "lm_ds = LMDataset(lm_texts[:200], tok)\n",
    "lm_dl = DataLoader(lm_ds, batch_size=16, shuffle=True, collate_fn=collate)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "losses = []\n",
    "for epoch in range(2):\n",
    "    for xb,yb in tqdm(lm_dl):\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"LM Training Loss\")\n",
    "plt.savefig(f\"{ASSETS_DIR}/lm_loss.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-tuning — Topic Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, base_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(base_model.head.out_features, num_classes)\n",
    "    def forward(self, x):\n",
    "        h = self.base(x)\n",
    "        pooled = h.mean(dim=1)\n",
    "        return self.fc(pooled)\n",
    "\n",
    "cls_model = Classifier(GPTMini(LM_VOCAB).to(device), num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClsDataset(Dataset):\n",
    "    def __init__(self, data, tok, max_len=64):\n",
    "        self.samples = []\n",
    "        for lab,text in data:\n",
    "            ids = tok.encode(text).ids[:max_len]\n",
    "            self.samples.append((ids,label_to_id[lab]))\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        ids,lab = self.samples[idx]\n",
    "        x = torch.tensor(ids, dtype=torch.long)\n",
    "        y = torch.tensor(lab, dtype=torch.long)\n",
    "        return x,y\n",
    "\n",
    "def collate_cls(batch,max_len=64):\n",
    "    xs,ys = zip(*batch)\n",
    "    maxlen = max(len(x) for x in xs)\n",
    "    X = torch.zeros(len(xs), maxlen, dtype=torch.long)\n",
    "    for i,x in enumerate(xs):\n",
    "        X[i,:len(x)] = x\n",
    "    Y = torch.tensor(ys)\n",
    "    return X,Y\n",
    "\n",
    "train_size = int(0.8*len(cls_data))\n",
    "val_size = len(cls_data)-train_size\n",
    "train_data,val_data = random_split(cls_data,[train_size,val_size])\n",
    "\n",
    "train_ds = ClsDataset(train_data, tok)\n",
    "val_ds = ClsDataset(val_data, tok)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=collate_cls)\n",
    "val_dl = DataLoader(val_ds, batch_size=16, shuffle=False, collate_fn=collate_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(cls_model.parameters(), lr=1e-3)\n",
    "losses_cls = []\n",
    "for epoch in range(3):\n",
    "    cls_model.train()\n",
    "    for xb,yb in train_dl:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        logits = cls_model(xb)\n",
    "        loss = F.cross_entropy(logits,yb)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        losses_cls.append(loss.item())\n",
    "\n",
    "plt.plot(losses_cls)\n",
    "plt.title(\"Classification Loss\")\n",
    "plt.savefig(f\"{ASSETS_DIR}/cls_loss.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation — Precision / Recall / F1 + Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model.eval()\n",
    "all_preds,all_true = [],[]\n",
    "with torch.no_grad():\n",
    "    for xb,yb in val_dl:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        logits = cls_model(xb)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_true.extend(yb.cpu().numpy())\n",
    "\n",
    "prec,rec,f1,_ = precision_recall_fscore_support(all_true, all_preds, average='weighted', zero_division=0)\n",
    "print(\"Weighted Precision:\", round(prec,3), \"Recall:\", round(rec,3), \"F1:\", round(f1,3))\n",
    "\n",
    "cm = confusion_matrix(all_true, all_preds, labels=list(range(num_classes)))\n",
    "plt.imshow(cm, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "for i in range(num_classes):\n",
    "    for j in range(num_classes):\n",
    "        plt.text(j,i,cm[i,j],ha='center',va='center')\n",
    "plt.xticks(range(num_classes), labels)\n",
    "plt.yticks(range(num_classes), labels)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{ASSETS_DIR}/cls_confusion.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Baseline: Bag-of-Words Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(max_features=5000,ngram_range=(1,2))\n",
    "X = vec.fit_transform([t for _,t in cls_data])\n",
    "y = np.array([label_to_id[l] for l,_ in cls_data])\n",
    "Xtr,Xva,ytr,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Demo — Try Classifier Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence(text: str) -> str:\n",
    "    ids = tok.encode(text).ids[:64]\n",
    "    ids = torch.tensor([ids], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = cls_model(ids)\n",
    "    pred = logits.argmax(dim=1).item()\n",
    "    return labels[pred]\n",
    "\n",
    "tests = [\n",
    "    \"The CET1 capital ratio fell from 13.1% to 12.9%.\",\n",
    "    \"Liquidity coverage ratios remained well above the 100% minimum.\",\n",
    "    \"Banks reduced leverage exposure through off-balance adjustments.\"\n",
    "]\n",
    "for s in tests:\n",
    "    print(f\"Sentence: {s}\")\n",
    "    print(\" → Predicted topic:\", classify_sentence(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Metrics for Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"precision\": float(round(prec,3)),\n",
    "    \"recall\": float(round(rec,3)),\n",
    "    \"f1\": float(round(f1,3)),\n",
    "    \"labels\": labels\n",
    "}\n",
    "with open(f\"{ASSETS_DIR}/metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics,f,indent=2)\n",
    "print(\"Metrics saved to assets/metrics.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
